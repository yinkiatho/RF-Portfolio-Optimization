{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "DATA_DIR = (os.getcwd() + \"/data/\")\n",
    "start_date = '1999-01-01'\n",
    "end_date = \"2019-12-31\"\n",
    "# Get fundamental data for each stock in the ticker and append to the dataframe\n",
    "\n",
    "\n",
    "def get_all_symbols():\n",
    "    return [v.strip('.csv') for v in os.listdir(DATA_DIR)]\n",
    "\n",
    "\n",
    "tickers = get_all_symbols()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Open</th>\n",
       "      <th>High</th>\n",
       "      <th>Low</th>\n",
       "      <th>Close</th>\n",
       "      <th>Adj Close</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Returns</th>\n",
       "      <th>Short Term Reversal</th>\n",
       "      <th>Stock Momentum</th>\n",
       "      <th>Long Term Reversal</th>\n",
       "      <th>Market_Beta</th>\n",
       "      <th>Turnover Volatility</th>\n",
       "      <th>Dividends</th>\n",
       "      <th>Total Returns</th>\n",
       "      <th>Total Return Volatility</th>\n",
       "      <th>Direction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1999-02-03</td>\n",
       "      <td>0.348214</td>\n",
       "      <td>0.362165</td>\n",
       "      <td>0.345982</td>\n",
       "      <td>0.358817</td>\n",
       "      <td>0.304563</td>\n",
       "      <td>338744000</td>\n",
       "      <td>1.025520</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>0.001558</td>\n",
       "      <td>-0.072151</td>\n",
       "      <td>1.064066</td>\n",
       "      <td>4.259781e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.025520</td>\n",
       "      <td>0.036932</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1999-02-04</td>\n",
       "      <td>0.358817</td>\n",
       "      <td>0.359375</td>\n",
       "      <td>0.337054</td>\n",
       "      <td>0.338170</td>\n",
       "      <td>0.287038</td>\n",
       "      <td>463780800</td>\n",
       "      <td>0.942458</td>\n",
       "      <td>-0.073393</td>\n",
       "      <td>-0.073393</td>\n",
       "      <td>-0.092814</td>\n",
       "      <td>1.243199</td>\n",
       "      <td>3.984316e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.057542</td>\n",
       "      <td>0.038231</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1999-02-05</td>\n",
       "      <td>0.341518</td>\n",
       "      <td>0.342634</td>\n",
       "      <td>0.316964</td>\n",
       "      <td>0.324219</td>\n",
       "      <td>0.275196</td>\n",
       "      <td>777201600</td>\n",
       "      <td>0.958746</td>\n",
       "      <td>-0.118362</td>\n",
       "      <td>-0.118362</td>\n",
       "      <td>-0.193056</td>\n",
       "      <td>1.299115</td>\n",
       "      <td>3.547813e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.041254</td>\n",
       "      <td>0.033808</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1999-02-08</td>\n",
       "      <td>0.327567</td>\n",
       "      <td>0.338728</td>\n",
       "      <td>0.323661</td>\n",
       "      <td>0.337054</td>\n",
       "      <td>0.286090</td>\n",
       "      <td>468227200</td>\n",
       "      <td>1.039588</td>\n",
       "      <td>-0.077860</td>\n",
       "      <td>-0.077860</td>\n",
       "      <td>-0.161111</td>\n",
       "      <td>1.329342</td>\n",
       "      <td>3.556811e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.039588</td>\n",
       "      <td>0.035544</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1999-02-09</td>\n",
       "      <td>0.338728</td>\n",
       "      <td>0.348772</td>\n",
       "      <td>0.330915</td>\n",
       "      <td>0.332031</td>\n",
       "      <td>0.281827</td>\n",
       "      <td>701153600</td>\n",
       "      <td>0.985097</td>\n",
       "      <td>-0.051036</td>\n",
       "      <td>-0.051036</td>\n",
       "      <td>-0.189373</td>\n",
       "      <td>1.237813</td>\n",
       "      <td>3.563124e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.014903</td>\n",
       "      <td>0.034968</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1999-02-10</td>\n",
       "      <td>0.329241</td>\n",
       "      <td>0.345424</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.342076</td>\n",
       "      <td>0.290353</td>\n",
       "      <td>563628800</td>\n",
       "      <td>1.030253</td>\n",
       "      <td>-0.046656</td>\n",
       "      <td>-0.046656</td>\n",
       "      <td>-0.169376</td>\n",
       "      <td>1.273495</td>\n",
       "      <td>3.527052e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.030253</td>\n",
       "      <td>0.035964</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1999-02-11</td>\n",
       "      <td>0.345982</td>\n",
       "      <td>0.354911</td>\n",
       "      <td>0.344308</td>\n",
       "      <td>0.353795</td>\n",
       "      <td>0.300300</td>\n",
       "      <td>565196800</td>\n",
       "      <td>1.034259</td>\n",
       "      <td>0.046205</td>\n",
       "      <td>0.046205</td>\n",
       "      <td>-0.147850</td>\n",
       "      <td>1.295339</td>\n",
       "      <td>3.359356e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.034258</td>\n",
       "      <td>0.037061</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1999-02-12</td>\n",
       "      <td>0.349330</td>\n",
       "      <td>0.349330</td>\n",
       "      <td>0.330357</td>\n",
       "      <td>0.336496</td>\n",
       "      <td>0.285617</td>\n",
       "      <td>428904000</td>\n",
       "      <td>0.951104</td>\n",
       "      <td>0.037866</td>\n",
       "      <td>0.037866</td>\n",
       "      <td>-0.089123</td>\n",
       "      <td>1.374354</td>\n",
       "      <td>1.975405e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.048896</td>\n",
       "      <td>0.029954</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1999-02-16</td>\n",
       "      <td>0.347098</td>\n",
       "      <td>0.347098</td>\n",
       "      <td>0.338170</td>\n",
       "      <td>0.342076</td>\n",
       "      <td>0.290353</td>\n",
       "      <td>300227200</td>\n",
       "      <td>1.016583</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>0.014900</td>\n",
       "      <td>-0.072618</td>\n",
       "      <td>1.384519</td>\n",
       "      <td>1.628264e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.016583</td>\n",
       "      <td>0.030311</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1999-02-17</td>\n",
       "      <td>0.340402</td>\n",
       "      <td>0.345424</td>\n",
       "      <td>0.329799</td>\n",
       "      <td>0.330357</td>\n",
       "      <td>0.280406</td>\n",
       "      <td>296060800</td>\n",
       "      <td>0.965742</td>\n",
       "      <td>-0.005042</td>\n",
       "      <td>-0.005042</td>\n",
       "      <td>-0.094801</td>\n",
       "      <td>1.414276</td>\n",
       "      <td>1.663125e+08</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.034259</td>\n",
       "      <td>0.031062</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date      Open      High       Low     Close  Adj Close     Volume  \\\n",
       "0  1999-02-03  0.348214  0.362165  0.345982  0.358817   0.304563  338744000   \n",
       "1  1999-02-04  0.358817  0.359375  0.337054  0.338170   0.287038  463780800   \n",
       "2  1999-02-05  0.341518  0.342634  0.316964  0.324219   0.275196  777201600   \n",
       "3  1999-02-08  0.327567  0.338728  0.323661  0.337054   0.286090  468227200   \n",
       "4  1999-02-09  0.338728  0.348772  0.330915  0.332031   0.281827  701153600   \n",
       "5  1999-02-10  0.329241  0.345424  0.321429  0.342076   0.290353  563628800   \n",
       "6  1999-02-11  0.345982  0.354911  0.344308  0.353795   0.300300  565196800   \n",
       "7  1999-02-12  0.349330  0.349330  0.330357  0.336496   0.285617  428904000   \n",
       "8  1999-02-16  0.347098  0.347098  0.338170  0.342076   0.290353  300227200   \n",
       "9  1999-02-17  0.340402  0.345424  0.329799  0.330357   0.280406  296060800   \n",
       "\n",
       "    Returns  Short Term Reversal  Stock Momentum  Long Term Reversal  \\\n",
       "0  1.025520             0.001558        0.001558           -0.072151   \n",
       "1  0.942458            -0.073393       -0.073393           -0.092814   \n",
       "2  0.958746            -0.118362       -0.118362           -0.193056   \n",
       "3  1.039588            -0.077860       -0.077860           -0.161111   \n",
       "4  0.985097            -0.051036       -0.051036           -0.189373   \n",
       "5  1.030253            -0.046656       -0.046656           -0.169376   \n",
       "6  1.034259             0.046205        0.046205           -0.147850   \n",
       "7  0.951104             0.037866        0.037866           -0.089123   \n",
       "8  1.016583             0.014900        0.014900           -0.072618   \n",
       "9  0.965742            -0.005042       -0.005042           -0.094801   \n",
       "\n",
       "   Market_Beta  Turnover Volatility  Dividends  Total Returns  \\\n",
       "0     1.064066         4.259781e+08        0.0       0.025520   \n",
       "1     1.243199         3.984316e+08        0.0      -0.057542   \n",
       "2     1.299115         3.547813e+08        0.0      -0.041254   \n",
       "3     1.329342         3.556811e+08        0.0       0.039588   \n",
       "4     1.237813         3.563124e+08        0.0      -0.014903   \n",
       "5     1.273495         3.527052e+08        0.0       0.030253   \n",
       "6     1.295339         3.359356e+08        0.0       0.034258   \n",
       "7     1.374354         1.975405e+08        0.0      -0.048896   \n",
       "8     1.384519         1.628264e+08        0.0       0.016583   \n",
       "9     1.414276         1.663125e+08        0.0      -0.034259   \n",
       "\n",
       "   Total Return Volatility  Direction  \n",
       "0                 0.036932         -1  \n",
       "1                 0.038231         -1  \n",
       "2                 0.033808          1  \n",
       "3                 0.035544         -1  \n",
       "4                 0.034968          1  \n",
       "5                 0.035964          1  \n",
       "6                 0.037061         -1  \n",
       "7                 0.029954          1  \n",
       "8                 0.030311         -1  \n",
       "9                 0.031062         -1  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_data.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yinki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "1600 fits failed out of a total of 4800.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "1600 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\yinki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py\", line 732, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"C:\\Users\\yinki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py\", line 1144, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"C:\\Users\\yinki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py\", line 637, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"C:\\Users\\yinki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "C:\\Users\\yinki\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:976: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53585484 0.53693885\n",
      " 0.53612216 0.53721206 0.53912307 0.53639537 0.5361251  0.53802728\n",
      " 0.53693811 0.54101493 0.54237142 0.54183015 0.53124264 0.53232223\n",
      " 0.53802728 0.53802654 0.53585484 0.53693885 0.53612216 0.53721206\n",
      " 0.53912307 0.53639537 0.5361251  0.53802728 0.53693811 0.54101493\n",
      " 0.54237142 0.54183015 0.53124264 0.53232223 0.53802728 0.53802654\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53775627 0.54019604\n",
      " 0.54101567 0.53802728 0.53857738 0.53449246 0.53829754 0.53884249\n",
      " 0.53693958 0.54074393 0.54155694 0.54183015 0.53124264 0.53232223\n",
      " 0.53802728 0.53802654 0.53775627 0.54019604 0.54101567 0.53802728\n",
      " 0.53857738 0.53449246 0.53829754 0.53884249 0.53693958 0.54074393\n",
      " 0.54155694 0.54183015 0.53124264 0.53232223 0.53802728 0.53802654\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53775627 0.54019604\n",
      " 0.54101567 0.53802728 0.53857738 0.53449246 0.53829754 0.53884249\n",
      " 0.53693958 0.54074393 0.54155694 0.54183015 0.53124264 0.53232223\n",
      " 0.53802728 0.53802654 0.53775627 0.54019604 0.54101567 0.53802728\n",
      " 0.53857738 0.53449246 0.53829754 0.53884249 0.53693958 0.54074393\n",
      " 0.54155694 0.54183015 0.53124264 0.53232223 0.53802728 0.53802654\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53775627 0.54019604\n",
      " 0.54101567 0.53802728 0.53857738 0.53449246 0.53829754 0.53884249\n",
      " 0.53693958 0.54074393 0.54155694 0.54183015 0.53124264 0.53232223\n",
      " 0.53802728 0.53802654 0.53775627 0.54019604 0.54101567 0.53802728\n",
      " 0.53857738 0.53449246 0.53829754 0.53884249 0.53693958 0.54074393\n",
      " 0.54155694 0.54183015 0.53124264 0.53232223 0.53802728 0.53802654\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53775627 0.54019604\n",
      " 0.54101567 0.53802728 0.53857738 0.53449246 0.53829754 0.53884249\n",
      " 0.53693958 0.54074393 0.54155694 0.54183015 0.53124264 0.53232223\n",
      " 0.53802728 0.53802654 0.53775627 0.54019604 0.54101567 0.53802728\n",
      " 0.53857738 0.53449246 0.53829754 0.53884249 0.53693958 0.54074393\n",
      " 0.54155694 0.54183015 0.53124264 0.53232223 0.53802728 0.53802654\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53748306 0.53775259\n",
      " 0.5374838  0.53884397 0.53911865 0.53313892 0.53693885 0.53857002\n",
      " 0.53585042 0.54155694 0.54237068 0.54128594 0.53151364 0.53612437\n",
      " 0.53639832 0.53829754 0.53748306 0.53775259 0.5374838  0.53884397\n",
      " 0.53911865 0.53313892 0.53693885 0.53857002 0.53585042 0.54155694\n",
      " 0.54237068 0.54128594 0.53151364 0.53612437 0.53639832 0.53829754\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53802654 0.5407432\n",
      " 0.53829975 0.53775922 0.53667521 0.53395045 0.53748085 0.5391135\n",
      " 0.53585115 0.5412852  0.54074172 0.54210042 0.53151364 0.53612437\n",
      " 0.53639832 0.53829754 0.53802654 0.5407432  0.53829975 0.53775922\n",
      " 0.53667521 0.53395045 0.53748085 0.5391135  0.53585115 0.5412852\n",
      " 0.54074172 0.54210042 0.53151364 0.53612437 0.53639832 0.53829754\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53802654 0.5407432\n",
      " 0.53829975 0.53775922 0.53667521 0.53395045 0.53748085 0.5391135\n",
      " 0.53585115 0.5412852  0.54074172 0.54210042 0.53151364 0.53612437\n",
      " 0.53639832 0.53829754 0.53802654 0.5407432  0.53829975 0.53775922\n",
      " 0.53667521 0.53395045 0.53748085 0.5391135  0.53585115 0.5412852\n",
      " 0.54074172 0.54210042 0.53151364 0.53612437 0.53639832 0.53829754\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53802654 0.5407432\n",
      " 0.53829975 0.53775922 0.53667521 0.53395045 0.53748085 0.5391135\n",
      " 0.53585115 0.5412852  0.54074172 0.54210042 0.53151364 0.53612437\n",
      " 0.53639832 0.53829754 0.53802654 0.5407432  0.53829975 0.53775922\n",
      " 0.53667521 0.53395045 0.53748085 0.5391135  0.53585115 0.5412852\n",
      " 0.54074172 0.54210042 0.53151364 0.53612437 0.53639832 0.53829754\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan 0.53802654 0.5407432\n",
      " 0.53829975 0.53775922 0.53667521 0.53395045 0.53748085 0.5391135\n",
      " 0.53585115 0.5412852  0.54074172 0.54210042 0.53151364 0.53612437\n",
      " 0.53639832 0.53829754 0.53802654 0.5407432  0.53829975 0.53775922\n",
      " 0.53667521 0.53395045 0.53748085 0.5391135  0.53585115 0.5412852\n",
      " 0.54074172 0.54210042 0.53151364 0.53612437 0.53639832 0.53829754]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'gini', 'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 100, 'n_estimators': 300}\n",
      "Accuracy on Test Data: 0.5345155161494617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.49      0.38      0.43       728\n",
      "           1       0.56      0.67      0.61       851\n",
      "\n",
      "    accuracy                           0.53      1579\n",
      "   macro avg       0.53      0.52      0.52      1579\n",
      "weighted avg       0.53      0.53      0.52      1579\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Random Forest Classifier Model to predict the direction of the stock\n",
    "ticker_data = pd.read_csv(DATA_DIR + 'AAPL.csv')\n",
    "ticker_data['Direction'] = np.where(ticker_data['Close'].shift(-1) > ticker_data['Close'], 1, -1)\n",
    "\n",
    "ticker_data.head(10)\n",
    "\n",
    "# Split the data into train and test set\n",
    "features = ticker_data[['Short Term Reversal', 'Adj Close', 'Volume',\n",
    "                        'Long Term Reversal', 'Stock Momentum', 'Market_Beta', 'Turnover Volatility', \n",
    "                        'Returns', 'Total Return Volatility', 'Total Returns']]\n",
    "target = ticker_data['Direction']\n",
    "X = np.array(features).reshape(-1,10)  # input as columns\n",
    "y = np.array(target)  # output as rows\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400],\n",
    "    'max_depth': [10,20, 30, 100, None],\n",
    "    'min_samples_leaf': [50, 75, 100, 150],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_features': ['auto', 'sqrt', 'log2']\n",
    "}\n",
    "model = RandomForestClassifier(random_state=42, n_jobs=-1, oob_score=True)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=model,\n",
    "                           param_grid=param_grid, cv=10)\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(grid_search.best_params_)\n",
    "# Print the cross-validation scores\n",
    "\n",
    "# Make predictions on the testing data\n",
    "grid_search_predictions = grid_search.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the testing data\n",
    "accuracy = accuracy_score(y_test, grid_search_predictions)\n",
    "print(\"Accuracy on Test Data:\", accuracy)\n",
    "\n",
    "# Generate a classification report to assess precision, recall, and F1-score\n",
    "print(classification_report(y_test, grid_search_predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = ['AAPL']\n",
    "for ticker in tickers:\n",
    "    ticker_data = pd.read_csv(f\"data/{ticker}.csv\", index_col=0)\n",
    "    ticker_data['Next Day Returns'] = ticker_data['Returns'].shift(-1)\n",
    "    ticker_data.dropna(inplace=True)\n",
    "    #print(ticker_data.head())\n",
    "    features = ticker_data[['Short Term Reversal', 'Adj Close', 'Volume', 'Long Term Reversal', 'Stock Momentum', 'Market_Beta', 'Turnover Volatility', \n",
    "                            'Returns', 'Total Return Volatility', 'Total Returns']]\n",
    "    target = ticker_data['Next Day Returns']\n",
    "    \n",
    "    X = np.array(features).reshape(-1, 10)  # input as columns\n",
    "    y = np.array(target)  # output as rows\n",
    "    \n",
    "    \n",
    "    \n",
    "    feature_train, feature_test, target_train, target_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "    #'''\n",
    "    #Building RF model\n",
    "    random_forest = RandomForestRegressor(n_jobs=-1, random_state=123, oob_score=True)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, 30, None],\n",
    "        'min_samples_leaf': [50, 75, 100],\n",
    "        'criterion': ['absolute_error', 'squared_error', 'friedman_mse'],\n",
    "        'max_features': [None, 'sqrt', 'log2'],\n",
    "    }   \n",
    "    \n",
    "    grid_search = GridSearchCV(estimator=random_forest, param_grid=param_grid, cv=5)\n",
    "    grid_search.fit(feature_train, target_train)\n",
    "    \n",
    "    print(grid_search.best_params_)\n",
    "    \n",
    "    criterion, max_depth, max_features, min_samples_leaf, n_estimators = grid_search.best_params_.values()\n",
    "    \n",
    "    #Building MLP model\n",
    "    mlp = MLPRegressor(max_iter=1000)\n",
    "    \n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(50, 50), (100, 50, 25), (100, 100)],\n",
    "        'activation': ['relu', 'tanh', 'identity'],\n",
    "        #'learning_rate': ['constant', 'adaptive'],\n",
    "        'alpha': [0.0001, 0.001, 0.01],\n",
    "    }\n",
    "\n",
    "    # Create a GridSearchCV instance\n",
    "    grid_search = GridSearchCV(mlp, param_grid, cv=5,\n",
    "                           n_jobs=-1, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Fit the GridSearchCV to find the best hyperparameters\n",
    "    grid_search.fit(feature_train, target_train)\n",
    "\n",
    "    # Get the best hyperparameters\n",
    "    best_params = grid_search.best_params_\n",
    "    print(best_params)\n",
    "    #'''\n",
    "    #base_models = [\n",
    "    #    ('rf', RandomForestRegressor(criterion='absolute_error', max_depth=10, max_features='log2', min_samples_leaf=100, n_estimators=200,\n",
    "    #                                 oob_score=True, random_state=123, n_jobs=-1)),\n",
    "    #    ('nn', MLPRegressor(max_iter=1000, activation='identity', alpha=0.001, hidden_layer_sizes=(100, 50, 25)))\n",
    "    #]\n",
    "    \n",
    "    base_models = [\n",
    "        ('rf', RandomForestRegressor(criterion=criterion, max_depth=max_depth, max_features=max_features, min_samples_leaf=min_samples_leaf, n_estimators=n_estimators,\n",
    "                                     oob_score=True, random_state=123, n_jobs=-1)),\n",
    "        ('nn', MLPRegressor(max_iter=1000, **best_params))\n",
    "    ]\n",
    "    \n",
    "    meta_learner = GradientBoostingRegressor(random_state=42, n_estimators=300, learning_rate=0.05, max_depth=6, max_features='sqrt')\n",
    "    #stack_model = StackingRegressor(estimators=base_models, final_estimator=meta_learner)\n",
    "    #Grid Search for meta learner\n",
    "    #param_grid = {\n",
    "    #    'final_estimator__n_estimators': [100, 200, 300],\n",
    "    #    'final_estimator__learning_rate': [0.01, 0.1, 0.2],\n",
    "    #    'final_estimator__max_depth': [3, 4, 5]  # Example parameter choices\n",
    "    #}\n",
    "\n",
    "    #grid_search = GridSearchCV(estimator=stack_model, param_grid=param_grid,\n",
    "    #                           cv=5, scoring='neg_mean_squared_error', n_jobs=-1)\n",
    "    #grid_search.fit(feature_train, target_train)\n",
    "    #print(grid_search.best_params_)\n",
    "    \n",
    "    #best_params = grid_search.best_params_\n",
    "    #best_meta_learner = grid_search.best_estimator_\n",
    "\n",
    "    \n",
    "    final_stack_model = StackingRegressor(estimators=base_models, final_estimator=meta_learner)\n",
    "    # Fit the stacking model on the training data\n",
    "    final_stack_model.fit(feature_train, target_train)\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    y_pred = final_stack_model.predict(feature_test)\n",
    "    \n",
    "   \n",
    "    #print(confusion_matrix(target_test, grid_search_predictions))\n",
    "    print(f\"Mean Absolute Error: {mean_absolute_error(target_test, y_pred)}\")\n",
    "    print(f\"Mean Squared Error: {mean_squared_error(target_test, y_pred)}\")\n",
    "    print(f\"R^2: {r2_score(target_test, y_pred)}\")\n",
    "    \n",
    "    #Add the predictions to the dataframe only for the test data, avoid look ahead bias\n",
    "    ticker_data['Next Day Returns Predictions'] = np.nan\n",
    "    ticker_data.iloc[-len(y_pred):, ticker_data.columns.get_loc('Next Day Returns Predictions')] = y_pred\n",
    "    ticker_data[['Next Day Returns', 'Next Day Returns Predictions']].plot(\n",
    "        figsize=(15, 5))\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Next Day Returns</th>\n",
       "      <th>Next Day Returns Predictions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-11-29</th>\n",
       "      <td>0.988438</td>\n",
       "      <td>0.999782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-02</th>\n",
       "      <td>0.982170</td>\n",
       "      <td>0.999432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-03</th>\n",
       "      <td>1.008826</td>\n",
       "      <td>1.000249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-04</th>\n",
       "      <td>1.014671</td>\n",
       "      <td>0.997803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-05</th>\n",
       "      <td>1.019316</td>\n",
       "      <td>1.006142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-06</th>\n",
       "      <td>0.986000</td>\n",
       "      <td>0.999136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-09</th>\n",
       "      <td>1.005844</td>\n",
       "      <td>1.002277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-10</th>\n",
       "      <td>1.008529</td>\n",
       "      <td>0.950992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-11</th>\n",
       "      <td>1.002548</td>\n",
       "      <td>1.000813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-12</th>\n",
       "      <td>1.013593</td>\n",
       "      <td>1.000537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-13</th>\n",
       "      <td>1.017118</td>\n",
       "      <td>1.004919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-16</th>\n",
       "      <td>1.001965</td>\n",
       "      <td>1.001317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-17</th>\n",
       "      <td>0.997611</td>\n",
       "      <td>1.005756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-18</th>\n",
       "      <td>1.001001</td>\n",
       "      <td>1.008232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-19</th>\n",
       "      <td>0.997929</td>\n",
       "      <td>1.001287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-20</th>\n",
       "      <td>1.016318</td>\n",
       "      <td>0.994234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-23</th>\n",
       "      <td>1.000951</td>\n",
       "      <td>0.999736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-24</th>\n",
       "      <td>1.019840</td>\n",
       "      <td>0.998152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-26</th>\n",
       "      <td>0.999621</td>\n",
       "      <td>1.004411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-27</th>\n",
       "      <td>1.005935</td>\n",
       "      <td>0.999955</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Next Day Returns  Next Day Returns Predictions\n",
       "Date                                                      \n",
       "2019-11-29          0.988438                      0.999782\n",
       "2019-12-02          0.982170                      0.999432\n",
       "2019-12-03          1.008826                      1.000249\n",
       "2019-12-04          1.014671                      0.997803\n",
       "2019-12-05          1.019316                      1.006142\n",
       "2019-12-06          0.986000                      0.999136\n",
       "2019-12-09          1.005844                      1.002277\n",
       "2019-12-10          1.008529                      0.950992\n",
       "2019-12-11          1.002548                      1.000813\n",
       "2019-12-12          1.013593                      1.000537\n",
       "2019-12-13          1.017118                      1.004919\n",
       "2019-12-16          1.001965                      1.001317\n",
       "2019-12-17          0.997611                      1.005756\n",
       "2019-12-18          1.001001                      1.008232\n",
       "2019-12-19          0.997929                      1.001287\n",
       "2019-12-20          1.016318                      0.994234\n",
       "2019-12-23          1.000951                      0.999736\n",
       "2019-12-24          1.019840                      0.998152\n",
       "2019-12-26          0.999621                      1.004411\n",
       "2019-12-27          1.005935                      0.999955"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ticker_data[['Next Day Returns', 'Next Day Returns Predictions']].tail(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\yinki\\OneDrive\\Python\\RF Portfolio Optimization\\RandomForest.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m100\u001b[39m, \u001b[39m200\u001b[39m, \u001b[39m300\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m10\u001b[39m, \u001b[39m15\u001b[39m, \u001b[39m20\u001b[39m, \u001b[39m30\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_features\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39mNone\u001b[39;00m, \u001b[39m'\u001b[39m\u001b[39msqrt\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlog2\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m }\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     estimator\u001b[39m=\u001b[39mrandom_forest, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(feature_train, target_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mprint\u001b[39m(grid_search\u001b[39m.\u001b[39mbest_params_)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/yinki/OneDrive/Python/RF%20Portfolio%20Optimization/RandomForest.ipynb#X15sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m grid_search_predictions \u001b[39m=\u001b[39m grid_search\u001b[39m.\u001b[39mpredict(feature_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:1419\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1418\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1419\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\parallel.py:127\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    125\u001b[0m     config \u001b[39m=\u001b[39m {}\n\u001b[0;32m    126\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconfig):\n\u001b[1;32m--> 127\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\model_selection\\_validation.py:732\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    730\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    731\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 732\u001b[0m         estimator\u001b[39m.\u001b[39mfit(X_train, y_train, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\n\u001b[0;32m    734\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    735\u001b[0m     \u001b[39m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    736\u001b[0m     fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\base.py:1151\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1144\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[1;32m-> 1151\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\ensemble\\_forest.py:456\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    445\u001b[0m trees \u001b[39m=\u001b[39m [\n\u001b[0;32m    446\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_estimator(append\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, random_state\u001b[39m=\u001b[39mrandom_state)\n\u001b[0;32m    447\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(n_more_estimators)\n\u001b[0;32m    448\u001b[0m ]\n\u001b[0;32m    450\u001b[0m \u001b[39m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[0;32m    451\u001b[0m \u001b[39m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[0;32m    452\u001b[0m \u001b[39m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[39m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[0;32m    454\u001b[0m \u001b[39m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[0;32m    455\u001b[0m \u001b[39m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[1;32m--> 456\u001b[0m trees \u001b[39m=\u001b[39m Parallel(\n\u001b[0;32m    457\u001b[0m     n_jobs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_jobs,\n\u001b[0;32m    458\u001b[0m     verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    459\u001b[0m     prefer\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mthreads\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    460\u001b[0m )(\n\u001b[0;32m    461\u001b[0m     delayed(_parallel_build_trees)(\n\u001b[0;32m    462\u001b[0m         t,\n\u001b[0;32m    463\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbootstrap,\n\u001b[0;32m    464\u001b[0m         X,\n\u001b[0;32m    465\u001b[0m         y,\n\u001b[0;32m    466\u001b[0m         sample_weight,\n\u001b[0;32m    467\u001b[0m         i,\n\u001b[0;32m    468\u001b[0m         \u001b[39mlen\u001b[39;49m(trees),\n\u001b[0;32m    469\u001b[0m         verbose\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mverbose,\n\u001b[0;32m    470\u001b[0m         class_weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mclass_weight,\n\u001b[0;32m    471\u001b[0m         n_samples_bootstrap\u001b[39m=\u001b[39;49mn_samples_bootstrap,\n\u001b[0;32m    472\u001b[0m     )\n\u001b[0;32m    473\u001b[0m     \u001b[39mfor\u001b[39;49;00m i, t \u001b[39min\u001b[39;49;00m \u001b[39menumerate\u001b[39;49m(trees)\n\u001b[0;32m    474\u001b[0m )\n\u001b[0;32m    476\u001b[0m \u001b[39m# Collect newly grown trees\u001b[39;00m\n\u001b[0;32m    477\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mestimators_\u001b[39m.\u001b[39mextend(trees)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:1098\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1095\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1097\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1098\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mretrieve()\n\u001b[0;32m   1099\u001b[0m \u001b[39m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1100\u001b[0m elapsed_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\parallel.py:975\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    973\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    974\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, \u001b[39m'\u001b[39m\u001b[39msupports_timeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m--> 975\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39;49mget(timeout\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtimeout))\n\u001b[0;32m    976\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    977\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output\u001b[39m.\u001b[39mextend(job\u001b[39m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\joblib\\_parallel_backends.py:567\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[39mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    566\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 567\u001b[0m     \u001b[39mreturn\u001b[39;00m future\u001b[39m.\u001b[39;49mresult(timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    568\u001b[0m \u001b[39mexcept\u001b[39;00m CfTimeoutError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    569\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\concurrent\\futures\\_base.py:441\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    438\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[0;32m    439\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__get_result()\n\u001b[1;32m--> 441\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_condition\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[0;32m    443\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n\u001b[0;32m    444\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n",
      "File \u001b[1;32mC:\\Program Files\\WindowsApps\\PythonSoftwareFoundation.Python.3.9_3.9.3568.0_x64__qbz5n2kfra8p0\\lib\\threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[0;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[0;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "tickers = ['AAPL']\n",
    "for ticker in tickers:\n",
    "    ticker_data = pd.read_csv(f\"data/{ticker}.csv\", index_col=0)\n",
    "    # print(ticker_data.head())\n",
    "    features = ticker_data[['Short Term Reversal', 'Adj Close', 'Volume',\n",
    "                            'Long Term Reversal', 'Stock Momentum', 'Market_Beta', 'Turnover Volatility']]\n",
    "    target = ticker_data['Returns']\n",
    "    \n",
    "    X = np.array(features).reshape(-1, 7)  # input as columns\n",
    "    y = np.array(target).reshape(-1, 1)  # output as rows\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    X = scaler.fit_transform(X)\n",
    "    y = scaler.fit_transform(y)\n",
    "    \n",
    "    y = y.ravel()\n",
    "\n",
    "\n",
    "    feature_train, feature_test, target_train, target_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=0)\n",
    "    random_forest = RandomForestRegressor(\n",
    "        n_jobs=-1, random_state=123, oob_score=True)\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [10, 15, 20, 30],\n",
    "        'min_samples_leaf': [50, 75, 100],\n",
    "        'criterion': ['absolute_error', 'squared_error', 'friedman_mse'],\n",
    "        'max_features': [None, 'sqrt', 'log2'],\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=random_forest, param_grid=param_grid, cv=10)\n",
    "    grid_search.fit(feature_train, target_train)\n",
    "\n",
    "    print(grid_search.best_params_)\n",
    "\n",
    "    grid_search_predictions = grid_search.predict(feature_test)\n",
    "\n",
    "    # print(confusion_matrix(target_test, grid_search_predictions))\n",
    "    print(mean_absolute_error(target_test, grid_search_predictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2016-01-01    0.924758\n",
      "2016-02-01    0.993322\n",
      "2016-03-01    1.133327\n",
      "2016-04-01    0.860079\n",
      "2016-05-01    1.065287\n",
      "2016-06-01    0.963169\n",
      "2016-07-01    1.090063\n",
      "2016-08-01    1.018137\n",
      "2016-09-01    1.071276\n",
      "2016-10-01    1.004334\n",
      "2016-11-01    0.973402\n",
      "2016-12-01    1.053335\n",
      "2017-01-01    1.047747\n",
      "2017-02-01    1.128884\n",
      "2017-03-01    1.053236\n",
      "2017-04-01    0.999930\n",
      "2017-05-01    1.063418\n",
      "2017-06-01    0.946678\n",
      "2017-07-01    1.032703\n",
      "2017-08-01    1.102670\n",
      "2017-09-01    0.943446\n",
      "2017-10-01    1.096808\n",
      "2017-11-01    1.016623\n",
      "2017-12-01    0.988294\n",
      "2018-01-01    0.989364\n",
      "2018-02-01    1.063848\n",
      "2018-03-01    0.945790\n",
      "2018-04-01    0.984980\n",
      "2018-05-01    1.130764\n",
      "2018-06-01    0.994402\n",
      "2018-07-01    1.027984\n",
      "2018-08-01    1.196227\n",
      "2018-09-01    0.995175\n",
      "2018-10-01    0.969522\n",
      "2018-11-01    0.815955\n",
      "2018-12-01    0.886384\n",
      "2019-01-01    1.055154\n",
      "2019-02-01    1.040315\n",
      "2019-03-01    1.101730\n",
      "2019-04-01    1.056436\n",
      "2019-05-01    0.872427\n",
      "2019-06-01    1.134873\n",
      "2019-07-01    1.076394\n",
      "2019-08-01    0.979816\n",
      "2019-09-01    1.077038\n",
      "2019-10-01    1.110684\n",
      "2019-11-01    1.074329\n",
      "2019-12-01    1.102083\n",
      "Name: Returns, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(target.iloc[int(len(target)* 0.8):])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
